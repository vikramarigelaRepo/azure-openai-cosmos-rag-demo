{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import urllib \n",
    "import gradio as gr\n",
    "\n",
    "from azure.core.exceptions import AzureError\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "#Cosmos DB imports\n",
    "from azure.cosmos import CosmosClient\n",
    "from azure.cosmos.aio import CosmosClient as CosmosAsyncClient\n",
    "from azure.cosmos import PartitionKey, exceptions\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables. \n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "env_name = \".env\" # following .env template change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "OPENAI_API_KEY = config['AZURE_OPENAI_KEY']\n",
    "OPENAI_API_ENDPOINT = config['AZURE_OPENAI_ENDPOINT']\n",
    "OPENAI_API_VERSION = config['AZURE_OPENAI_VERSION'] # at the time of authoring, the api version is 2024-02-01\n",
    "COMPLETIONS_MODEL_DEPLOYMENT_NAME = config['AZURE_OPENAI_COMPLETIONS_DEPLOYMENT']\n",
    "EMBEDDING_MODEL_DEPLOYMENT_NAME = config['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT']\n",
    "COSMOSDB_NOSQL_ACCOUNT_KEY = config['COSMOSDB_KEY']\n",
    "COSMOSDB_NOSQL_ACCOUNT_ENDPOINT = config['COSMOSDB_URI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize OpenAI Client\n",
    "AOAI_client = AzureOpenAI(api_key=OPENAI_API_KEY, azure_endpoint=OPENAI_API_ENDPOINT, api_version=OPENAI_API_VERSION,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to create embeddings using azure open ai\n",
    "def generate_embeddings(text):\n",
    "    '''\n",
    "    Generate embeddings from string of text.\n",
    "    This will be used to vectorize data and user input for interactions with Azure OpenAI.\n",
    "    '''\n",
    "    response = AOAI_client.embeddings.create(input=text, model=EMBEDDING_MODEL_DEPLOYMENT_NAME)\n",
    "    embeddings =response.model_dump()\n",
    "    time.sleep(0.5) \n",
    "    return embeddings['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data file\n",
    "data =[]\n",
    "with open('text-sample.json', 'r') as d:\n",
    "    data = json.load(d)\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for title and content fields\n",
    "n = 0\n",
    "for item in data:\n",
    "    n+=1\n",
    "    item['id'] = str(n)\n",
    "    title = item['title']\n",
    "    content = item['content']\n",
    "    title_embeddings = generate_embeddings(title)\n",
    "    content_embeddings = generate_embeddings(content)\n",
    "    item['titleVector'] = title_embeddings\n",
    "    item['contentVector'] = content_embeddings\n",
    "    item['@search.action'] = 'upload'\n",
    "    print(\"Creating embeddings for item:\", n, \"/\" ,len(data), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save embeddings to sample_text_w_embeddings.json file\n",
    "with open(\"text-sample_w_embeddings.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Cosmos Client\n",
    "cosmos_client = CosmosClient(url=COSMOSDB_NOSQL_ACCOUNT_ENDPOINT, credential=COSMOSDB_NOSQL_ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create database\n",
    "DATABASE_NAME = \"vector-nosql-db\"\n",
    "db= cosmos_client.create_database_if_not_exists(\n",
    "    id=DATABASE_NAME\n",
    ")\n",
    "properties = db.read()\n",
    "print(json.dumps(properties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector embedding policy\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [\n",
    "        {\n",
    "            \"path\":\"/titleVector\",\n",
    "            \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"dotproduct\",\n",
    "            \"dimensions\":1536\n",
    "        },\n",
    "        {\n",
    "            \"path\":\"/contentVector\",\n",
    "            \"dataType\":\"float32\",\n",
    "            \"distanceFunction\":\"cosine\",\n",
    "            \"dimensions\":1536\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector indexing policy\n",
    "indexing_policy = {\n",
    "    \"includedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/*\"\n",
    "        }\n",
    "    ],\n",
    "    \"excludedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/\\\"_etag\\\"/?\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/titleVector/*\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/contentVector/*\"\n",
    "        }\n",
    "    ],\n",
    "    \"vectorIndexes\": [\n",
    "        {\"path\": \"/titleVector\",\n",
    "         \"type\": \"quantizedFlat\"\n",
    "        },\n",
    "        {\"path\": \"/contentVector\",\n",
    "         \"type\": \"quantizedFlat\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create container and cache container for semantic caching\n",
    "CONTAINER_NAME = \"vector-nosql-cont\"\n",
    "CACHE_CONTAINER_NAME = \"vector-nosql-cache\"\n",
    "\n",
    "\n",
    "try:    \n",
    "    container = db.create_container_if_not_exists(\n",
    "                    id=CONTAINER_NAME,\n",
    "                    partition_key=PartitionKey(path='/id', kind='Hash'),\n",
    "                    indexing_policy=indexing_policy,\n",
    "                    vector_embedding_policy=vector_embedding_policy)\n",
    "\n",
    "    print('Container with id \\'{0}\\' created'.format(id))\n",
    "\n",
    "except exceptions.CosmosResourceExistsError:\n",
    "    print('A container with id \\'{0}\\' already exists'.format(id))\n",
    "\n",
    "\n",
    "# Create the cache collection with vector index\n",
    "try:\n",
    "    cache_container = db.create_container_if_not_exists(id=CACHE_CONTAINER_NAME, \n",
    "                                                  partition_key=PartitionKey(path='/id'), \n",
    "                                                  indexing_policy=indexing_policy,\n",
    "                                                  vector_embedding_policy=vector_embedding_policy)\n",
    "    print('Container with id \\'{0}\\' created'.format(cache_container.id)) \n",
    "\n",
    "except exceptions.CosmosHttpResponseError: \n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize container client.\n",
    "CONTAINER_NAME = \"vector-nosql-cont\"\n",
    "CACHE_CONTAINER_NAME = \"vector-nosql-cache\"\n",
    "\n",
    "container = db.get_container_client(CONTAINER_NAME)\n",
    "cache_container = db.get_container_client(CACHE_CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert data and embeddings into cosmos db.\n",
    "\n",
    "with open('text-sample_w_embeddings.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "container_client = db.get_container_client(CONTAINER_NAME)\n",
    "\n",
    "for item in data:\n",
    "  print(\"writing item\",item['id'])\n",
    "  container_client.upsert_item(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets chat history from cache container.\n",
    "def get_chat_history(container, completions=3):\n",
    "    results = container.query_items(\n",
    "        query= '''\n",
    "        SELECT TOP @completions *\n",
    "        FROM c\n",
    "        ORDER BY c._ts DESC\n",
    "        ''',\n",
    "        parameters=[\n",
    "            {\"name\": \"@completions\", \"value\": completions},\n",
    "        ],enable_cross_partition_query=True)\n",
    "    results = list(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the vector search by running sample query\n",
    "query = \"What are the services for running ML models?\"\n",
    "results = vector_search(query)\n",
    "for result in results: \n",
    "  #print(result)\n",
    "    print(f\"Similarity Score: {result['SimilarityScore']}\")\n",
    "    print(f\"patientId: {result['title']}\")  \n",
    "    print(f\"patientId: {result['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function helps to ground the model with prompts and system instructions.\n",
    "\n",
    "def generate_completion(vector_search_results, user_prompt, chat_history):\n",
    "    system_prompt = '''\n",
    "    You are an intelligent assistant for Microsoft Azure services.\n",
    "    You are designed to provide helpful answers to user questions about Azure services given the information about to be provided.\n",
    "        - Only answer questions related to the information provided below, provide at least 3 clear suggestions in a list format.\n",
    "        - Write two lines of whitespace between each answer in the list.\n",
    "        - If you're unsure of an answer, you can say \"\"I don't know\"\" or \"\"I'm not sure\"\" and recommend users search themselves.\"\n",
    "        - Only provide answers that have products that are part of Microsoft Azure and part of these following prompts.\n",
    "    '''\n",
    "\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "        #chat history\n",
    "    for chat in chat_history:\n",
    "        messages.append({'role': 'user', 'content': chat['prompt'] + \" \" + chat['completion']})\n",
    "\n",
    "    for item in vector_search_results:\n",
    "        messages.append({\"role\": \"system\", \"content\": item['content']})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    response = AOAI_client.chat.completions.create(model=COMPLETIONS_MODEL_DEPLOYMENT_NAME, messages=messages,temperature=0)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to cache response for semantic caching\n",
    "import uuid\n",
    "def cache_response(container, user_prompt, prompt_vectors, response):\n",
    "    # Create a dictionary representing the chat document\n",
    "    chat_document = {\n",
    "        'id':  str(uuid.uuid4()),  \n",
    "        'prompt': user_prompt,\n",
    "        'completion': response.choices[0].message.content,\n",
    "        'completionTokens': str(response.usage.completion_tokens),\n",
    "        'promptTokens': str(response.usage.prompt_tokens),\n",
    "        'totalTokens': str(response.usage.total_tokens),\n",
    "        'model': response.model,\n",
    "        'vector': prompt_vectors\n",
    "    }\n",
    "    # Insert the chat document into the Cosmos DB container\n",
    "    container.create_item(body=chat_document)\n",
    "    print(\"item inserted into cache.\", chat_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a vector search on the Cosmos DB Cache container\n",
    "def get_cache(container, vectors, similarity_score=0.0, num_results=5):\n",
    "    # Execute the query\n",
    "    results = container.query_items(\n",
    "        query= '''\n",
    "        SELECT TOP @num_results *\n",
    "        FROM c\n",
    "        WHERE VectorDistance(c.vector,@embedding) > @similarity_score\n",
    "        ORDER BY VectorDistance(c.vector,@embedding)\n",
    "        ''',\n",
    "        parameters=[\n",
    "            {\"name\": \"@embedding\", \"value\": vectors},\n",
    "            {\"name\": \"@num_results\", \"value\": num_results},\n",
    "            {\"name\": \"@similarity_score\", \"value\": similarity_score},\n",
    "        ],\n",
    "        enable_cross_partition_query=True, populate_query_metrics=True)\n",
    "    results = list(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction for chat completion\n",
    "\n",
    "def chat_completion(cache_container,user_input):\n",
    "   # container = db.get_container_client(CONTAINER_NAME)\n",
    "    cache_container = db.get_container_client(CACHE_CONTAINER_NAME)\n",
    "    #while user_input.lower() != \"end\":\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "\n",
    "   # Query the chat history cache first to see if this question has been asked before\n",
    "    cache_results = get_cache(container = cache_container, vectors = user_embeddings, similarity_score=0.99, num_results=1)\n",
    "    if len(cache_results) > 0:\n",
    "        print(\"Cached Result\\n\")\n",
    "        return cache_results[0]['completion'], True\n",
    "   \n",
    "    else: \n",
    "\n",
    "      print(\"New result\\n\")\n",
    "      search_results = vector_search(user_input)\n",
    "      #chat history\n",
    "      chat_history = get_chat_history(cache_container, 3)\n",
    "\n",
    "      completions_results = generate_completion(search_results, user_input,chat_history)\n",
    "      #completions_results = generate_completion(search_results, user_input)\n",
    "\n",
    "      print(\"\\n\")\n",
    "\n",
    "      print(\"Caching response \\n\")\n",
    "      #cache the response\n",
    "      cache_response(cache_container, user_input, user_embeddings, completions_results)\n",
    "\n",
    "      return completions_results.choices[0].message.content, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block which used gradio to spin up a simple chat like UI\n",
    "chat_history = []\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Azure Assistant\")\n",
    "    \n",
    "    msg = gr.Textbox(label=\"Ask me about Azure Services!\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, chat_history):\n",
    "        # Create a timer to measure the time it takes to complete the request\n",
    "        start_time = time.time()\n",
    "        # Get LLM completion\n",
    "        response_payload, cached = chat_completion(cache_container, user_message)\n",
    "        # Stop the timer\n",
    "        end_time = time.time()\n",
    "        elapsed_time = round((end_time - start_time) * 1000, 2)\n",
    "        #response = response_payload\n",
    "        print(response_payload)\n",
    "        # Append user message and response to chat history\n",
    "        details = f\"\\n (Time: {elapsed_time}ms)\"\n",
    "        if cached:\n",
    "         details += \" (Cached)\"\n",
    "        chat_history.append([user_message, response_payload + details])\n",
    "        \n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# Launch the Gradio interface\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
